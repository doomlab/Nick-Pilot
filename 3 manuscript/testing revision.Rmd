---
title: "testing revision"
author: "Erin M. Buchanan"
date: "6/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import both data

```{r}
pilot = read.csv("data/Melted Data pilot.csv", stringsAsFactors = F)
thesis = read.csv("data/Melted Data Thesis.csv", stringsAsFactors = F)

thesis$Partno = thesis$Partno + 1000
pilot$where = "pilot"
thesis$where = "thesis"

colnames(pilot)[2] = "Judgment"

master = rbind(pilot, thesis)

nrow(master)
length(unique(master$Partno))
```

## Merge with SWOW

```{r}
swow = read.csv("data/strength.SWOW-EN.R123.csv", stringsAsFactors = F)

swow$pair = paste(swow$cue, swow$response, sep = "_")
swow$pair = tolower(swow$pair)
swow$cue = tolower(swow$cue)
swow$response = tolower(swow$response)
swow = swow[-c(1001283) , ] #remove the dup

master$pair = gsub("...", "_", master$Word.Pair, fixed = T)
master$pair = tolower(master$pair)

master2 = merge(master, swow, by = "pair", all.x = T)
nrow(master2)
##red yellow is duplicating - removing the duplicate line from swow

psych::describe(master2$R123.Strength)

#normalize SWOW as the score is not number of participants who said X / number of participants, but instead is number of participants who said X / number of answers
#normalized = (x-min(x)) / (max(x) - min(x))

table(master2[is.na(master2$R123.Strength) , "pair"])

master2$swow_fsg = (master2$R123.Strength - min(master2$R123.Strength, na.rm = T)) / (max(master2$R123.Strength, na.rm = T) - min(master2$R123.Strength, na.rm = T))

master2$swow_fsg[is.na(master2$swow_fsg)] = 0

psych::describe(master2$swow_fsg)

plot(master2$FSG, master2$swow_fsg)
```

Here we would treat SWOW as the "first order associations" described in the reviewers suggestion. 

## Create the 2nd order

Comparing the distributions of associative norms = finding the cosine between each cue-target words response sets. 

```{r}
library(lsa)

paired = unique(master2$pair)
paired = as.data.frame(paired)
paired$paired = as.character(paired$paired)
paired$cosine = NA

for (i in 1:length(paired$paired)){ #loop over each word
  
  words = strsplit(paired$paired[i], "_")
  temp1 = swow[tolower(swow$cue) == words[[1]][1], ]
  temp2 = swow[tolower(swow$cue) == words[[1]][2], ]
  
  temp_merge = merge(temp1, temp2, by = "response", all = T)
  temp_merge$R123.Strength.x[is.na(temp_merge$R123.Strength.x)] = 0
  temp_merge$R123.Strength.y[is.na(temp_merge$R123.Strength.y)] = 0
  
  paired$cosine[i] = lsa::cosine(temp_merge$R123.Strength.x, temp_merge$R123.Strength.y)

}

#merge with  our data
colnames(paired)[1] = "pair"
master3 = merge(paired, master2, by = "pair")
```

## Let's compare our variables

- The SWOW fsg is pretty similar to the regular FSG ... check. 
- Unrelated to the distribution of the responses, interesting (cosine).
- COS (ours) to cosine (distribution) is pretty high, that's good.
- LSA here is the same relation to swow_fsg as before FSG, and less related to this new cosine. 

```{r}
vars = c("swow_fsg", "cosine", "COS", "LSA", "FSG")
cor(master3[ , vars])

#scaling judgments
master3$Judged.Value = master3$Judged.Value / 100

#clean up judgment type
master3$Judgment = gsub("semantic", "Semantic", master3$Judgment)
master3$Judgment = gsub("associative", "Associative", master3$Judgment)
master3$Judgment = gsub("thematic", "Thematic", master3$Judgment)
```

## Hypothesis 1

```{r}
##are average slope and intercept values greater than zero?
##are they within the range of previous work, although maybe controlling for other variables might change them a bit
##frequency of strongest predictor 

####try a loop####
##setup
persontable = matrix(NA,
                     nrow=length(names(table(master3$Partno))),
                     ncol=10)
colnames(persontable) = c("Partno", "AIntercept", "ACOS", "AFSG",
                          "SIntercept", "SCOS", "SFSG",
                          "TIntercept", "TCOS", "TFSG")
simnum = 1

for ( person in names(table(master3$Partno)) ){ ##loop over participants
  
  temp1 = subset(master3, Partno == person & Judgment == "Associative")
  temp2 = subset(master3, Partno == person & Judgment == "Semantic")
  temp3 = subset(master3, Partno == person & Judgment == "Thematic")
  
  persontable[ simnum , 1] = person
  
  if (sum(!is.na(temp1$Judged.Value)) > 9) {
    model9 = lm(Judged.Value ~ cosine + swow_fsg, data = temp1)
    persontable[ simnum , 2:4] = model9$coefficients
  }
  
  if(sum(!is.na(temp2$Judged.Value)) > 9) {
    model10 = lm(Judged.Value ~ cosine + swow_fsg, data = temp2)
    persontable[ simnum , 5:7] = model10$coefficients
  }
  
  if(sum(!is.na(temp3$Judged.Value)) > 9) {
    model11 = lm(Judged.Value ~ cosine + swow_fsg, data = temp3)
    persontable[ simnum , 8:10] = model11$coefficients
  }
  
  simnum = simnum + 1
  
}

##set up the output
people = apply(persontable, 2, as.numeric)
people = as.data.frame(people) 

####hyp 1 are these greater than zero####
##single sample t-test for each column, focus on the effect size
library(MOTE)
library(knitr)

p.value = function(x){
  if (x < .001) { return("< .001")}
  else { return(apa(x, 3, F))}
}

hyp1results = apply(people[ , -1], 2, function (x) { d.single.t(m = mean(x, na.rm = T), 
                                u = 0,
                                sd = sd(x, na.rm = T),
                                n = sum(!is.na(x)),
                                a = .05)})

##what happened? make a table
##create a blank table
tableprint = matrix(NA, nrow = 9, ncol = 8)
colnames(tableprint) = c("Variable", "$M$", "$SD$", "$t$", "$df$", "$p$", "$d$", "$95 CI$")

tableprint[1 , ] = c("Associative Intercept", apa(hyp1results$AIntercept$m, 3, F),
                     apa(hyp1results$AIntercept$sd, 3, F), apa(hyp1results$AIntercept$t, 3, T),
                     hyp1results$AIntercept$df, p.value(hyp1results$AIntercept$p),
                     apa(hyp1results$AIntercept$d, 3, T), paste(apa(hyp1results$AIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$AIntercept$dhigh, 3, T)))

tableprint[2 , ] = c("Associative COS", apa(hyp1results$ACOS$m, 3, F),
                     apa(hyp1results$ACOS$sd, 3, F), apa(hyp1results$ACOS$t, 3, T),
                     hyp1results$ACOS$df, p.value(hyp1results$ACOS$p),
                     apa(hyp1results$ACOS$d, 3, T), paste(apa(hyp1results$ACOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$ACOS$dhigh, 3, T)))

tableprint[3 , ] = c("Associative FSG", apa(hyp1results$AFSG$m, 3, F),
                     apa(hyp1results$AFSG$sd, 3, F), apa(hyp1results$AFSG$t, 3, T),
                     hyp1results$AFSG$df, p.value(hyp1results$AFSG$p),
                     apa(hyp1results$AFSG$d, 3, T), paste(apa(hyp1results$AFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$AFSG$dhigh, 3, T)))

tableprint[4 , ] = c("Semantic Intercept", apa(hyp1results$SIntercept$m, 3, F),
                     apa(hyp1results$SIntercept$sd, 3, F), apa(hyp1results$SIntercept$t, 3, T),
                     hyp1results$SIntercept$df, p.value(hyp1results$SIntercept$p),
                     apa(hyp1results$SIntercept$d, 3, T), paste(apa(hyp1results$SIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$SIntercept$dhigh, 3, T)))

tableprint[5 , ] = c("Semantic COS", apa(hyp1results$SCOS$m, 3, F),
                     apa(hyp1results$SCOS$sd, 3, F), apa(hyp1results$SCOS$t, 3, T),
                     hyp1results$SCOS$df, p.value(hyp1results$SCOS$p),
                     apa(hyp1results$SCOS$d, 3, T), paste(apa(hyp1results$SCOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$SCOS$dhigh, 3, T)))

tableprint[6 , ] = c("Semantic FSG", apa(hyp1results$SFSG$m, 3, F),
                     apa(hyp1results$SFSG$sd, 3, F), apa(hyp1results$SFSG$t, 3, T),
                     hyp1results$SFSG$df, p.value(hyp1results$SFSG$p),
                     apa(hyp1results$SFSG$d, 3, T), paste(apa(hyp1results$SFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$SFSG$dhigh, 3, T)))

tableprint[7 , ] = c("Thematic Intercept", apa(hyp1results$TIntercept$m, 3, F),
                     apa(hyp1results$TIntercept$sd, 3, F), apa(hyp1results$TIntercept$t, 3, T),
                     hyp1results$TIntercept$df, p.value(hyp1results$TIntercept$p),
                     apa(hyp1results$TIntercept$d, 3, T), paste(apa(hyp1results$TIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$TIntercept$dhigh, 3, T)))

tableprint[8 , ] = c("Thematic COS", apa(hyp1results$TCOS$m, 3, F),
                     apa(hyp1results$TCOS$sd, 3, F), apa(hyp1results$TCOS$t, 3, T),
                     hyp1results$TCOS$df, p.value(hyp1results$TCOS$p),
                     apa(hyp1results$TCOS$d, 3, T), paste(apa(hyp1results$TCOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$TCOS$dhigh, 3, T)))

tableprint[9 , ] = c("Thematic FSG", apa(hyp1results$TFSG$m, 3, F),
                     apa(hyp1results$TFSG$sd, 3, F), apa(hyp1results$TFSG$t, 3, T),
                     hyp1results$TFSG$df, p.value(hyp1results$TFSG$p),
                     apa(hyp1results$TFSG$d, 3, T), paste(apa(hyp1results$TFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$TFSG$dhigh, 3, T)))

kable(tableprint)
```

Comparison to original results:
- Intercepts all pretty much the same over estimation effect
- Assoc FSG > stronger / higher than before 
- Assoc cosine about the same
- Semantic FSG > way stronger than before, makes a lot more damned sense
- BECAUSE semantic cosine is now a thing!
- Thematic fsg is about the same but now makes more sense becausee
- Thematic cosine is a thing!

This result tells me:
- interecepts are what we expect .4-.6
- slopes are mostly what we expect
- slopes matter by instruction!

## Hypothesis 2

```{r}
##group them all by type
##dropping partno column
people_thematic = people[ , 8:10]
people_semantic = people[ , 5:7]
people_associative = people[ , 2:4]

##getting absolute values
people_thematic2 = abs(people_thematic)
people_semantic2 = abs(people_semantic)
people_associative2 = abs(people_associative)

##getting rid of the intercepts
people_thematic3 = people_thematic2[ , -1]
people_semantic3 = people_semantic2[ , -1]
people_associative3 = people_associative2[ , -1]

##finding max coefficents for each row
thematic_output = names(people_thematic3)[max.col(people_thematic3, ties.method="first")]
thematic_output = as.data.frame(thematic_output)

semantic_output = names(people_semantic3)[max.col(people_semantic3, ties.method="first")]
semantic_output = as.data.frame(semantic_output)

associative_output = names(people_associative3)[max.col(people_associative3, ties.method="first")]
associative_output = as.data.frame(associative_output)

##sticking it all together
combined = cbind(people$Partno, associative_output, semantic_output, thematic_output)
colnames(combined) = c("Partno", "Associative", "Semantic", "Thematic")

##rename the things
combined$Associative = factor(combined$Associative,
                           labels = c("COS", "FSG"))
combined$Semantic = factor(combined$Semantic,
                           labels = c("COS", "FSG"))
combined$Thematic = factor(combined$Thematic,
                           labels = c("COS", "FSG"))
summary(combined[ , -1])

##what are the percentages on these?
library(memisc)
p1 = percent(combined$Associative)
p1
p2 = percent(combined$Semantic)
p2
p3 = percent(combined$Thematic)
p3

```
- Previously FSG predicted assoc at 64 now 67
- FSG for semantic was highest with  44, now we are seeing even split
- FSG for thematic was 37 with basically nothing for cosine now seeing more even split

## A two level interaction for Hypothesis 2 for JOR

```{r}
library(nlme)

#center the vars
master3$zfsg = scale(master3$swow_fsg, scale = F)
master3$zcosine = scale(master3$cosine, scale = F)

overallh2 = lme(Judged.Value ~ Judgment + 
               zfsg*zcosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2)
```

## What is the interaction

```{r}
#split on cosine because that's what we did before
master3$low_cosine = master3$zcosine + sd(master3$zcosine)
master3$high_cosine = master3$zcosine - sd(master3$zcosine)

overallh2_low = lme(Judged.Value ~ Judgment + 
               zfsg*low_cosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2_low) ##.38

summary(overallh2) ##.30

overallh2_high = lme(Judged.Value ~ Judgment + 
               zfsg*high_cosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2_high) ##.22 
```

So, what we see is at low levels of cosine, FSG is pulling the weight, but as cosine increases, the effect of FSG decreases (see-sawing).

## Recall for Hypothesis 3

```{r}
library(lme4)

recalloverall = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*zcosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(recalloverall) ##1.63

recalloverall_low = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*low_cosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)

summary(recalloverall_low) #1.89

recalloverall_high = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*high_cosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)

summary(recalloverall_high) #1.37
```

Find the exact same pattern!

## Hypothesis 4

```{r}
##merge recall back with slopes from the people dataset
hyp4data = merge(people, master3, by = "Partno")

##association
assoch4 = glmer(Recall ~ (1|Partno) + ACOS + AFSG + AIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##semantics
semh4 = glmer(Recall ~ (1|Partno) + SCOS + SFSG + SIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##thematics
themeh4 = glmer(Recall ~ (1|Partno) + TCOS + TFSG + TIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

####table set up####
##create a blank table
tableprint = matrix(NA, nrow = 12, ncol = 5)
colnames(tableprint) = c("Variable", "$b$", "$SE$", "$z$", "$p$")

tableprint[1:4, 2:5] = coef(summary(assoch4))
tableprint[1:4, 1] = dimnames(coef(summary(assoch4)))[[1]]

tableprint[5:8, 2:5] = coef(summary(semh4))
tableprint[5:8, 1] = dimnames(coef(summary(semh4)))[[1]]

tableprint[9:12, 2:5] = coef(summary(themeh4))
tableprint[9:12, 1] = dimnames(coef(summary(themeh4)))[[1]]

library(papaja)
tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] = apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)

kable(tableprint)
```

- Association: we found before that fsg slope predicts best then cosine 
  - Now seeing fsg with no cosine
- Semantics: we found before that cosine predicts best then fsg
  - now seeing same thing 
- Thematics: cosine then negative fsg
  - now seeing fsg then cosine 
  
# TLDR

- Similiar results with maybe less confusion on variables, not mixing types, and no three way interactions!
- Combine thesis and pilot data
  - Still hang on to thesis ideas for part 2 looking at single word variables contributions for a second paper (if you want)
- Switch to this idea of a direct versus indirect relationship?


####YOU START HERE#####



Additionally, we examined the frequency that each predictor variable was the strongest predictor for each of the three JOR conditions. For the associative condition, FSG was the strongest predictor for `r apa(p1[2],1)`% of the participants, with COS and LSA being the strongest for only `r apa(p1[1],1)`% and `r apa(p1[3],1)`% of participants respectively. These differences were less distinct when examining the semantic and thematic JOR conditions. In the semantic condition, FSG was highest at `r apa(p2[2], 1)`% of participants, LSA was second at `r apa(p2[3],1)`%, and COS was least likely at `r apa(p2[1],1)`%. Finally, in the thematic condition, LSA was most likely to be the strongest predictor with `r apa(p3[3],1)`% of participants, with FSG being the second most likely at `r apa(p3[2],1)`%, and COS again being least likely at `r apa(p3[1],1)`%. Interestingly, in all three conditions, COS was least likely to be the strongest predictor, even in the semantic condition. Therefore, these results provide evidence of the nature of judgments on the memory network as each judgment type appeared to tap each tier differently, suggesting a three-part system, rather than one large, encompassing memory network. 

## Interaction between Norms when Predicting Judgments of Relatedness
```{r hyp2-table, echo=FALSE, results='asis'}
##interaction in predicting judgments, use MLM to control for random participant intercepts 

####mean center the predictor variables####
noout$ZCOS = scale(noout$COS, scale = F)
noout$ZLSA = scale(noout$LSA, scale = F)
noout$ZFSG = scale(noout$FSG, scale = F)

##create the right scaling 
noout$Judged.Value2 = noout$Judged.Value/100

####overall model####
library(nlme)
library(MuMIn)
overallh2 = lme(Judged.Value2 ~ Judgment + 
                  ZCOS * ZLSA * ZFSG, 
                data = noout, 
                method = "ML", 
                na.action = "na.omit",
                random = ~1|Partno)
#summary(overallh2) ##everything is significant
#r.squaredGLMM(overallh2) ##use R2m to talk about variance predicted





####table set up####
##create a blank table
tableprint = matrix(NA, nrow = 22, ncol = 5)
colnames(tableprint) = c("Variable", "$beta$", "$SE$", "$t$", "$p$")

tableprint[1:10 , 2:5] = summary(overallh2)$tTable[ , -3]
tableprint[1:10, 1] = c("Intercept", "Semantic Judgments", "Thematic Judgments", dimnames(summary(overallh2)$tTable[ , -3])[[1]][4:10] )

####examine three way interaction by breaking down the COS variable first####
##setup
noout$ZCOS_low = noout$ZCOS + sd(noout$ZCOS, na.rm = TRUE)
noout$ZCOS_high = noout$ZCOS - sd(noout$ZCOS, na.rm = TRUE)

##low cosine
lowcos = lme(Judged.Value2 ~ Judgment +
               ZCOS_low * ZLSA * ZFSG, 
             data = noout, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)
#summary(lowcos) ##two way interaction is significant
tableprint[11:13 , 2:5] = summary(lowcos)$tTable[ c(5,6,9), -3]
tableprint[11:13, 1] = c("Low COS ZLSA", 
                         "Low COS ZFSG", 
                         "Low COS ZLSA:ZFSG")

##high cosine
highcos = lme(Judged.Value2 ~ Judgment +
                ZCOS_high * ZLSA * ZFSG, 
              data = noout, 
              method = "ML", 
              na.action = "na.omit",
              random = ~1|Partno)
#summary(highcos) ##two way interaction is significant

tableprint[14:16 , 2:5] = summary(highcos)$tTable[ c(5,6,9), -3]
tableprint[14:16, 1] = c("High COS ZLSA", 
                         "High COS ZFSG", 
                         "High COS ZLSA:ZFSG")


####examine the two way interactions by splitting LSA####
##setup
noout$ZLSA_low = noout$ZLSA + sd(noout$ZLSA, na.rm = TRUE)
noout$ZLSA_high = noout$ZLSA - sd(noout$ZLSA, na.rm = TRUE)

##low cosine, low lsa
lowcoslowlsa = lme(Judged.Value2 ~ Judgment +
                     ZCOS_low * ZLSA_low * ZFSG, 
                   data = noout, 
                   method = "ML", 
                   na.action = "na.omit",
                   random = ~1|Partno)
#summary(lowcoslowlsa) ##fsg is sig
tableprint[17, ] = c("Low COS Low LSA ZFSG",
                     summary(lowcoslowlsa)$tTable[6, -3])

##low cos, avg lsa is low cosine overall model

##low cos, high lsa
lowcoshighlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS_low  * ZLSA_high * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
#summary(lowcoshighlsa) ##fsg is not sig
tableprint[18, ] = c("Low COS High LSA ZFSG",
                     summary(lowcoshighlsa)$tTable[6, -3])

##avg cos, low lsa
acgcoslowlsa = lme(Judged.Value2 ~ Judgment +
                     ZCOS * ZLSA_low * ZFSG, 
                   data = noout, 
                   method = "ML", 
                   na.action = "na.omit",
                   random = ~1|Partno)
#summary(acgcoslowlsa) ##fsg is sig
tableprint[19, ] = c("Avg COS Low LSA ZFSG",
                     summary(acgcoslowlsa)$tTable[6, -3])

##avg cos, avg lsa is overall model overallh2

##avg cos, high lsa
avgcoshighlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS  * ZLSA_high * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
#summary(avgcoshighlsa) ##fsg is sig
tableprint[20, ] = c("Avg COS High LSA ZFSG",
                     summary(avgcoshighlsa)$tTable[6, -3])

##high cos, low lsa
highcoslowlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS_high * ZLSA_low * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
#summary(highcoslowlsa) ##fsg is not sig
tableprint[21, ] = c("High COS Low LSA ZFSG",
                     summary(highcoslowlsa)$tTable[6, -3])

##high cos, avg lsa is the overall high cosine model

##high cos, high lsa
highcoshighlsa = lme(Judged.Value2 ~ Judgment +
                       ZCOS_high  * ZLSA_high * ZFSG, 
                     data = noout, 
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
#summary(highcoshighlsa) ##fsg is sig
tableprint[22, ] = c("High COS High LSA ZFSG",
                     summary(highcoshighlsa)$tTable[6, -3])
tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] =  apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)

apa_table(as.data.frame(tableprint),
          align = c("l", rep("c", 4)), 
          small = T,
          caption = "MLM Statistics for Hypothesis 2",
          note = "Database norms were mean centered. The table shows main effects and interactions for database norms at low, average, and high levels of COS and LSA when predicting participant judgments.",
          escape = FALSE
)

```

```{r hyp2graph, echo=FALSE, fig.cap = "Simple slopes graph displaying the slope of FSG when predicting JORs at low, average, and high LSA split by low, average, and high COS. All variables were mean centered.", fig.height=6, fig.width=6}
##hyp 2 graphs
####setup####
dat = noout

##low cos
plot1 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgments") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = .607, slope = .663, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = .632, slope = .375, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = .657, slope = .087, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") 

##avg cos
plot2 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgments") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = .586, slope = .381, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = .603, slope = .271, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = .621, slope = .161, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS") 

##high cos
plot3 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgments") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = .564, slope = .099, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = .575, slope = .167, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = .586, slope = .236, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

# arrange plots together
legend = get_legend(plot1)
hyp2graphout <- plot_grid( plot1 + theme(legend.position="none"),
                           plot2 + theme(legend.position="none"),
                           plot3 + theme(legend.position="none"),
                           legend,
                           hjust = -1,
                           nrow = 2
)
hyp2graphout

#tiff(filename = "hyp2graph.tiff", res = 300, width = 6, 
#     height = 6, units = 'in', compression = "lzw")
#plot(hyp2graphout)
#dev.off()
```

The goal of next analysis was to test for an interaction between the three overlap norms when predicting participant JORs to examine the bidirectional network model. First, the database norms were mean centered to control for multicollinearity. The *nlme* package and *lme* function were used to calculate these analyses [@Pinheiro2017]. A maximum likelihood multilevel model was used to test the interaction between FSG, COS, and LSA when predicting JOR values, with participant number used as the random intercept factor. The type of JOR being elicited was controlled for, so as to better assess the impact of each word overlap measure regardless of JOR condition. Multilevel models were used to retain all data points (rather than averaging over items and conditions) while controlling for correlated error due to participants, which makes these models advantageous for multiway repeated measures designs [@Gelman2006]. This analysis resulted in a significant three-way interaction between FSG, COS, and LSA ($\beta$ = `r tableprint[10 , 2]`, *p* `r tableprint[10 , 5]`), which is examined below in a simple slopes analysis. Table \@ref(tab:hyp2-table) includes values for main effects, two-way, and three-way interactions.

To investigate this interaction, simple slopes were calculated for low, average, and high levels of COS. This variable was chosen for two reasons: first, it was found to be the weakest of the three predictors in hypothesis one, and second, manipulating COS would allow us to track changes across FSG and LSA. Significant two-way interactions were found between FSG and LSA at both low COS ($\beta$ = `r tableprint[13 , 2]`, *p* `r tableprint[13 , 5]`), average COS ($\beta$ =`r tableprint[9 , 2]`, *p* `r tableprint[9 , 5]`), and high COS ($\beta$ = `r tableprint[16 , 2]`, *p* = `r tableprint[16 , 5]`). A second level was then added to the analysis in which simple slopes were created for each level of LSA, allowing us to assess the effects of LSA at different levels of COS on FSG. When both COS and LSA were low, FSG significantly predicted JOR values ($\beta$ = `r tableprint[17 , 2]`, *p* `r tableprint[17 , 5]`). At low COS and average LSA, FSG decreased but still significantly predicted JORs ($\beta$ = `r tableprint[12 , 2]`, *p* `r tableprint[12 , 5]`). However, when COS was low and LSA was high, FSG was not a significant predictor ($\beta$ = `r tableprint[18 , 2]`, *p* = `r tableprint[18 , 5]`). A similar set of results was found at the average COS level. When COS was average and LSA was LOW, FSG was a significant predictor, ($\beta$ = `r tableprint[19 , 2]`, *p* `r tableprint[19 , 5]`). As LSA increased at average COS levels, FSG decreased in strength: average COS, average LSA FSG ($\beta$ = `r tableprint[16 , 2]`, *p* = `r tableprint[16 , 5]`) and average COS, high LSA FSG ($\beta$ = `r tableprint[20 , 2]`, *p* `r tableprint[20 , 5]`). This finding suggests that at low COS, LSA and FSG create a seesaw effect in which increasing levels of thematics is counterbalanced by decreasing importance of association when predicting JORs. FSG was not a significant predictor when COS was high and LSA was low (`r tableprint[21 , 2]`, *p* = `r tableprint[21, 5]`). At high COS and average LSA, FSG significantly predicted JORs ($\beta$ = `r tableprint[15 , 2]`, *p* `r tableprint[15 , 5]`), and finally when both COS and LSA were high, FSG increased and was a significant predictor of JOR values ($\beta$ = `r tableprint[22 , 2]`, *p* `r tableprint[22 , 5]`). Thus, at high levels of semantic overlap, associative and thematic overlap are complementary when predicting JOR ratings, increasing together as semantic strength increases. Figure \@ref(fig:hyp2graph) displays the three-way interaction wherein the top row of figures indicates the seesaw effect, as thematic strength increases, the predictive ability of associative overlap decreases in strength. The bottom row indicates the complementary effect where increases in LSA occur with increases in FSG predictor strength. Therefore, the cognitive process of judgment appears to be interactive in nature across these three types of memory information.

## Interaction between Norms when Predicting Recall
```{r hyp3-table, echo=FALSE, results='asis'}
##old hypothesis 1
##using recall as the DV, CV judgment type, rating, IVs three database norms
##use zscores created for hyp 2

##overall model
library(lme4)
overallh3 = glmer(Recall ~ (1|Partno) + Judgment + 
                    Judged.Value2 + ZCOS * ZLSA * ZFSG,
                  data = noout,
                  family = binomial,
                  control = glmerControl(optimizer = "bobyqa"),
                  nAGQ = 1)

#summary(overallh3)
#r.squaredGLMM(overallh3)

####table set up####
##create a blank table
tableprint = matrix(NA, nrow = 21, ncol = 5)
colnames(tableprint) = c("Variable", "$beta$", "$SE$", "$z$", "$p$")

tableprint[1:11 , 2:5] = coef(summary(overallh3))
tableprint[1:11, 1] = c("Intercept", "Semantic Judgments", "Thematic Judgments", "Judged Values", dimnames(coef(summary(overallh3)))[[1]][5:11])

#low cosine
lowcos2 = glmer(Recall ~ (1|Partno) +
                  Judgment + Judged.Value2 + ZCOS_low * ZLSA * ZFSG,
                data = noout,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

tableprint[12:14 , 2:5] = coef(summary(lowcos2))[c(6,7,10), ]
tableprint[12:14, 1] = c("Low COS ZLSA", 
                         "Low COS ZFSG", 
                         "Low COS ZLSA:ZFSG")

##high cosine
hicos2 = glmer(Recall ~ (1|Partno) + 
                 Judgment + Judged.Value2 + ZCOS_high * ZLSA * ZFSG,
               data = noout,
               family = binomial,
               control = glmerControl(optimizer = "bobyqa"),
               nAGQ = 1)

tableprint[15:17, 2:5] = coef(summary(hicos2))[c(6,7,10), ]
tableprint[15:17, 1] = c("High COS ZLSA", 
                         "High COS ZFSG", 
                         "High COS ZLSA:ZFSG")

####splitting lsa by cosine strength####
##low cosine low lsa
lowcoslowlsa2 = glmer(Recall ~ (1|Partno) + 
                        Judgment + Judged.Value2 + ZCOS_low * ZLSA_low * ZFSG,
                      data = noout,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 1)

tableprint[18, ] = c("Low COS Low LSA ZFSG", coef(summary(lowcoslowlsa2))[7,])

##low cosine high lsa
lowcoshighlsa2 = glmer(Recall ~ (1|Partno) + 
                         Judgment + Judged.Value2 + ZCOS_low * ZLSA_high * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1)

tableprint[19, ] = c("Low COS High LSA ZFSG", coef(summary(lowcoshighlsa2))[7,])

##high cos low lsa
highcoslowlsa2 = glmer(Recall ~ (1|Partno) + 
                         Judgment + Judged.Value2 + ZCOS_high * ZLSA_low * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1)

tableprint[20, ] = c("High COS Low LSA ZFSG", coef(summary(highcoslowlsa2))[7,])

##high cosine high lsa
highcoshighlsa2 = glmer(Recall ~ (1|Partno) + 
                          Judgment + Judged.Value2 + ZCOS_high * ZLSA_high * ZFSG,
                        data = noout,
                        family = binomial,
                        control = glmerControl(optimizer = "bobyqa"),
                        nAGQ = 1)

tableprint[21, ] = c("High COS High LSA ZFSG", coef(summary(highcoshighlsa2))[7,])

tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] = apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)


apa_table(as.data.frame(tableprint),
          align = c("l", rep("c", 4)), 
          small = T,
          caption = "MLM Statistics for Hypothesis 3",
          note = "Database norms were mean centered. The table shows main effects and interactions for database norms at low, average, and high levels of COS and LSA when predicting recall.",
          escape = FALSE
)

```

```{r hyp3graph, echo=FALSE, fig.cap = "Simple slopes graph displaying the slope of FSG when predicting recall at low, average, and high LSA split by low, average, and high COS. All variables were mean centered.", fig.height=6, fig.width=6}

##low cos
plot4 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.316, slope = 4.116, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.136, slope = 2.601, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = -0.044, slope = 1.086, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") 

##avg cos
plot5 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.369, slope = 3.281, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.301, slope = 3.084, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.234, slope = 2.889, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS") 

##high cos
plot6 = ggplot(dat, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = .421, slope = 2.44, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = .466, slope = 3.569, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = .511, slope = 4.692, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

legend = get_legend(plot4)
hyp3graphout <- plot_grid( plot4 + theme(legend.position="none"),
                           plot5 + theme(legend.position="none"),
                           plot6 + theme(legend.position="none"),
                           legend,
                           hjust = -1,
                           nrow = 2
)
hyp3graphout

#tiff(filename = "hyp3graph.tiff", res = 300, width = 6, 
#     height = 6, units = 'in', compression = "lzw")
#plot(hyp3graphout)
#dev.off()

```

Given the results of Hypothesis 2, we then sought to extend the analysis to participant recall scores. A multilevel logistic regression was used with the *lme4* package and *glmer()* function [@Pinheiro2017], testing the interaction between FSG, COS, and LSA when predicting participant recall. As with the previous hypothesis, we controlled for JOR condition and, additionally, covaried JOR ratings. Participants were used as a random intercept factor. Judged values were a significant predictor of recall, ($\beta$ = `r tableprint[4, 2]`, *p* `r tableprint[4, 5]`) where increases in judged strength predicted increases in recall. A significant three-way interaction was detected between FSG, COS, and LSA ($\beta$ = `r tableprint[11, 2]`, *p* `r tableprint[11, 5]`). See Table \@ref(tab:hyp3-table) for main effects, two-way, and three-way interaction values. 

The same moderation process used in Hypothesis 2 was then repeated, with simple slopes first calculated at low, average, and high levels of COS. This set of analyses resulted in significant two-way interactions between LSA and FSG at low COS ($\beta$ = `r tableprint[14, 2]`, *p* `r tableprint[14, 5]`) and high COS ($\beta$ = `r tableprint[17, 2]`, *p* = `r tableprint[17, 5]`). No significant two-way interaction was found at average COS ($\beta$ = `r tableprint[10, 2]`, *p* = `r tableprint[10, 5]`). Following the design used when predicting JORs, simple slopes were then calculated for low, average, and high levels of LSA at the low and high levels of COS, allowing us to assess how FSG affects recall at varying levels of both COS and LSA. When both COS and LSA were low, FSG was a significant predictor of recall ($\beta$ = `r tableprint[18, 2]`, *p* `r tableprint[18, 5]`). At low COS and average LSA, FSG decreased from both low levels, but was still a significant predictor ($\beta$ = `r tableprint[13, 2]`, *p* `r tableprint[13, 5]`), and finally, low COS and high LSA, FSG was the weakest predictor of the three ($\beta$ = `r tableprint[19, 2]`, *p* = `r tableprint[19, 5]`). As with Hypothesis 2, LSA and FSG counterbalanced one another, wherein the increasing levels of thematics led to a decrease in the importance of association in predicting recall. At high COS and low LSA, FSG was a significant predictor ($\beta$ = `r tableprint[20, 2]`, *p* = `r tableprint[20, 5]`). When COS was high and LSA was average, FSG increased as a predictor and remained significant ($\beta$ = `r tableprint[16, 2]`, *p* `r tableprint[16, 5]`). This finding repeated when both COS and LSA were high, with FSG increasing as a predictor of recall ($\beta$ = `r tableprint[21, 2]`, *p* `r tableprint[21, 5]`). Therefore, at high levels of at high levels of semantics, thematics and association are complementary predictors of recall, increasing together and extending the findings of Hypothesis 2 to participant recall. Figure \@ref(fig:hyp3graph) displays the three-way interaction. The top left figure indicates the counterbalancing effect of recall of LSA and FSG, while the top right figure shows no differences in simple slopes for average levels of cosine. The bottom left figure indicates the complementary effects where LSA and FSG increase together as predictors of recall at high COS levels. 

## Predicting Recall with JAM Slopes

```{r hyp4-table, echo=FALSE, results='asis'}
##do the slopes from hyp 1 predict recall 

##merge recall back with slopes from the people dataset
hyp4data = merge(people, noout, by = "Partno")

##association
assoch4 = glmer(Recall ~ (1|Partno) + ACOS + ALSA + AFSG + AIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##semantics
semh4 = glmer(Recall ~ (1|Partno) + SCOS + SLSA + SFSG + SIntercept,
              data = hyp4data,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

##thematics
themeh4 = glmer(Recall ~ (1|Partno) + TCOS + TLSA + TFSG + TIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

####table set up####
##create a blank table
tableprint = matrix(NA, nrow = 15, ncol = 5)
colnames(tableprint) = c("Variable", "$b$", "$SE$", "$z$", "$p$")

tableprint[1:5, 2:5] = coef(summary(assoch4))
tableprint[1:5, 1] = dimnames(coef(summary(assoch4)))[[1]]

tableprint[6:10, 2:5] = coef(summary(semh4))
tableprint[6:10, 1] = dimnames(coef(summary(semh4)))[[1]]

tableprint[11:15, 2:5] = coef(summary(themeh4))
tableprint[11:15, 1] = dimnames(coef(summary(themeh4)))[[1]]

tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] = apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)

apa_table(as.data.frame(tableprint),
          align = c("l", rep("c", 4)), 
          caption = "MLM Statistics for Hypothesis 4",
          note = "Each judgment-database bias and sensitivity predicting recall for corresponding judgment block. A: Associative, S: Semantic, T: Thematic.",
          escape = FALSE
)

```

In our fourth and final hypothesis, we investigated whether the JOR slopes and intercepts obtained in Hypothesis 1 would be predictive of recall ability. Whereas Hypothesis 3 indicated that word relatedness was directly related to recall performance, this hypothesis instead looked at whether or not participants' sensitivity and bias to word relatedness could be used a predictor of recall [@Maki2007]. This analysis was conducted with a multilevel logistic regression, as described in Hypothesis 3, where each database slope and intercept was used as predictors of recall using participant as a random intercept factor. These analyses were separated by judgment condition, so that each set of JOR slopes and intercepts were used to predict recall. The separation controlled for the number of variables in the equation, as all slopes and intercepts would have resulted in overfitting. These values were obtained from Hypothesis 1 where each participant's individual slopes and intercepts were calculated for associative, semantic, and thematic JOR conditions. Table \@ref(tab:hyp1-table1) shows average slopes and intercepts for recall for each of the three types of memory, and Table \@ref(tab:hyp4-table) portrays the regression coefficients and statistics. In the associative condition, FSG slope significantly predicted recall (*b* = `r tableprint[4, 2]`, *p* = `r tableprint[4, 5]`), while COS slope (*b* = `r tableprint[2, 2]`, *p* = `r tableprint[2, 5]`) and LSA slope (*b* = `r tableprint[3, 2]`, *p* = `r tableprint[3, 5]`) were non-significant. In the semantic condition, COS slope (*b* = `r tableprint[7, 2]`, *p* `r tableprint[7, 5]`) and LSA slope (*b* = `r tableprint[8, 2]`, *p* = `r tableprint[8, 5]`) were both found to be significant predictors of recall. FSG slope was non-significant in this condition (*b* = `r tableprint[9, 2]`, *p* = `r tableprint[9, 5]`). Finally, no predictors were significant in the thematic condition, though LSA slope was found to be the strongest (*b* = `r tableprint[13, 2]`, *p* = `r tableprint[13, 5]`). This analysis indicated the extent to which the cognitive processes are related to each other as part of the memory network (i.e., judgment sensitivity predicting recall), furthering the previous two analyses, which illustrated the nature of those cognitive processes' relationship with the underlying memory network. 

# Discussion

This study investigated the relationship between associative, semantic, and thematic word relations and their effect on participant JORs and recall performance through the testing of four hypotheses. In our first hypothesis, we show that bias and sensitivity findings first proposed by @Maki2007a successfully replicated in the associative condition, with slope and intercept values falling within the expected range. While these findings were not fully replicated when extending the analysis to include semantic and thematic JORs (as slopes in these conditions did not fall within the appropriate range), participants still displayed high intercepts and shallow slopes, suggesting overconfidence in judgment making and an insensitivity to changes in strength between pairs. Additionally, when looking at the frequency that each predictor was the strongest in making JORs, FSG was the best predictor for both the associative and semantic conditions, while LSA was the best predictor in the thematic condition. In each of the three conditions, COS was the weakest predictor, even when participants were asked to make semantic judgments. This finding suggests that associative relationships seem to take precedence over semantic relationships when judging pair relatedness, regardless of what type of JOR is being elicited. Additionally, this finding may be taken as further evidence of a separation between associative information and semantic information, in which associative information is always processed, while semantic information may be suppressed due to task demands [@Buchanan2010; @Hutchison2007]. 

Our second hypothesis examined the three-way interaction between FSG, COS, and LSA when predicting participant JORs. At low semantic overlap, a seesaw effect was found in which increases in thematic strength led to decreases in associative predictiveness. This finding was then replicated in Hypothesis 3 when extending the analysis to predict recall. By limiting the semantic relationships between pairs, an increased importance is placed on the role of associations and thematics when making relatedness judgments or retrieving pairs. In such cases, increasing the amount of thematic overlap between pairs results in thematic relationships taking precedent over associative relationships. However, when semantic overlap was high, a complementary relationship was found in which increases in thematic strength in turn led to increases in the strength of FSG as a predictor. This result suggests that at high semantic overlap, associations and thematic relations build upon one another. Because thematics is tied to both semantic overlap and item associations, the presence of strong thematic relationships between pairs during conditions of high semantic overlap boosts the predictive ability of associative word norms for both recall and JORs. 

Finally, our fourth hypothesis used the JOR slopes and intercepts calculated in Hypothesis 1 to investigate whether participants' bias and sensitivity to word relatedness could be used to predict recall. For the associative condition, the FSG slope significantly predicted recall. In the semantic condition, recall was significantly predicted by both the COS and LSA slopes, with COS being the strongest. However, for the thematic condition, although the LSA slope was the strongest, no predictors were significant. One explanation for this finding is that thematic relationships between item pairs act as a blend between associations and semantics. As such, LSA faces increased competition from the associative and semantic database norms when predicting recall in this manner. Additionally, the dominance of FSG when predicting recall in the associative condition may be attributed to word associations being more accessible (and, thus, easier to process) than semantic or thematic relations between pairs.

Overall, our findings indicated the degree to which the processing of associative, semantic, and thematic information impacts retrieval and judgment making tasks and the interactive relationship that exists between these three types of lexical information. While previous research has shown that memory networks are divided into separate systems which handle storage and processing for meaning and association [see @Ferrand2004 for a review], the presence of these interactions suggests that connections exist between these individual memory networks, linking them to one another. As such, we suggest that these memory systems may be connected in such a way as to form a three-tiered, interconnected system. Within this framework, information enters the semantic memory network, which processes features of concepts and provides a means of categorizing items based on the similarity of their features. The associative network adds information for items based on contexts generated by reading or speech. The thematic network then pulls in information based on semantic and associative input to create a mental representation of both the item and its place in the world relative to other concepts.

While this study did not explore the timing or order in which information is input from each of these systems of concept information, it may be comparable to the dual-route model of reading and naming [@Coltheart1993], in that each system runs in parallel when contributing to the judgment and recall process [See also @Coltheart2001]. Alternatively, prevous research on early linguistic access suggests that these three systems may be operating simultaneously. For example, The Language and Situated Simulation Theory (LASS, see @Barsalou2008 for a review) proposes that meaning is derived from the concurrent processing of both linguistic information (e.g., visual stimuli) and modal simulations (i.e., mental imagery related to the stimulus object). According to this theory, when an object is read, both systems become imemediately engaged, though the linguistic system is assumed to be faster, because mental representations of linguistic forms are more accessible in memory relative to simulations (Barsalou et al., 2008, @Thomson1973). As the linguistic system becomes activated, it in turn activates related linguistic forms (i.e., associated words). Because word associations play a key role within this linguistic system, associative information may then come online first relative to semantic concept information.

Viewing this model purely through the lens of semantic memory, it draws comparison to dynamic attractor models [@Jones2015; @Hopfield1982; @McLeod2000]. One of the defining features of dynamic attractor models is that they allow for some type of bidirectionally or feedback between connections in the network. In the study of semantic memory, these models are useful for taking into account multiple restraints such as links between semantics and the orthography of the concept in question. Our hypothesis extends this notion as a means of framing how these three memory systems are connected. The underlying meaning of a concept is linked with both information pertaining to its co-occurrences in everyday language and information relating to the general contexts in which it typically appears.

How then does this hypothesis lend itself towards the broader context of psycholinguistic research? One application of this hypothesis may be found when examining models of word recognition. One popular class of word recognition models are those based upon the "triangle model" first proposed by @Seidenberg1989a [see @Harley2008 for a review]. The key feature of these models is that they recognize speech and reading based upon the orthography, phonology, and meaning of words in a bidirectional manner, similar to the models described above. @Harm2004 developed a version which included a focus on semantics, with word meaning being related to input from the orthography and phonology components of the model. As our findings from the present study suggest that thematic and associative knowledge is integrated with meaning, one way of framing our results within this literature is to consider the semantic section of the triangle model as being comprised of this interconnected system,with concept information processed to some degree on each of these domains. Thus, one area for future studies of this nature may be investigating how aspects of orthography and phonology impact these memory networks. 

Finally, future studies may wish to consider elements of thematic and associative knowledge when examining semantic based tasks (such as word recognition and reading), as thematic and associative information is interconnected with the semantic network. To fully understand the interplay of associatitive, semantic, and thematic networks, it will be necessary to control for several types of word properties (e.g., item frequency, concreteness, etc.), as these properties have been shown to influence judgments and recall. Ultimately, our findings show that these three lexical networks are highly related suggest they are individual components of an interconnected system.

## Compliance with Ethical Standards

The authors declare that they have no conflict of interest. The study was approved by the Institutional Review Board at Missouri State University. Particiants filled out an informed consent at the beginning of the study, after accepting the HIT on Mechanical Turk. The complete study with consent form can be found on our OSF page: http://osf.io/y8h7v.


