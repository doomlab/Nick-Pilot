---
title: "testing revision"
author: "Erin M. Buchanan"
date: "6/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import both data

```{r}
pilot = read.csv("Melted Data pilot.csv", stringsAsFactors = F)
thesis = read.csv("Melted Data Thesis.csv", stringsAsFactors = F)

thesis$Partno = thesis$Partno + 1000
pilot$where = "pilot"
thesis$where = "thesis"

colnames(pilot)[2] = "Judgment"

master = rbind(pilot, thesis)

nrow(master)
```

## Merge with SWOW

```{r}
swow = read.csv("strength.SWOW-EN.R123.csv", stringsAsFactors = F)

swow$pair = paste(swow$cue, swow$response, sep = "_")
swow$pair = tolower(swow$pair)
swow$cue = tolower(swow$cue)
swow$response = tolower(swow$response)
swow = swow[-c(1001283) , ] #remove the dup

master$pair = gsub("...", "_", master$Word.Pair, fixed = T)
master$pair = tolower(master$pair)

master2 = merge(master, swow, by = "pair", all.x = T)
nrow(master2)
##red yellow is duplicating - removing the duplicate line from swow

psych::describe(master2$R123.Strength)

#normalize SWOW as the score is not number of participants who said X / number of participants, but instead is number of participants who said X / number of answers
#normalized = (x-min(x)) / (max(x) - min(x))

table(master2[is.na(master2$R123.Strength) , "pair"])

master2$swow_fsg = (master2$R123.Strength - min(master2$R123.Strength, na.rm = T)) / (max(master2$R123.Strength, na.rm = T) - min(master2$R123.Strength, na.rm = T))

master2$swow_fsg[is.na(master2$swow_fsg)] = 0

psych::describe(master2$swow_fsg)

plot(master2$FSG, master2$swow_fsg)
```

Here we would treat SWOW as the "first order associations" described in the reviewers suggestion. 

## Create the 2nd order

Comparing the distributions of associative norms = finding the cosine between each cue-target words response sets. 

```{r}
library(lsa)

paired = unique(master2$pair)
paired = as.data.frame(paired)
paired$paired = as.character(paired$paired)
paired$cosine = NA

for (i in 1:length(paired$paired)){ #loop over each word
  
  words = strsplit(paired$paired[i], "_")
  temp1 = swow[tolower(swow$cue) == words[[1]][1], ]
  temp2 = swow[tolower(swow$cue) == words[[1]][2], ]
  
  temp_merge = merge(temp1, temp2, by = "response", all = T)
  temp_merge$R123.Strength.x[is.na(temp_merge$R123.Strength.x)] = 0
  temp_merge$R123.Strength.y[is.na(temp_merge$R123.Strength.y)] = 0
  
  paired$cosine[i] = lsa::cosine(temp_merge$R123.Strength.x, temp_merge$R123.Strength.y)

}

#merge with  our data
colnames(paired)[1] = "pair"
master3 = merge(paired, master2, by = "pair")
```

## Let's compare our variables

- The SWOW fsg is pretty similar to the regular FSG ... check. 
- Unrelated to the distribution of the responses, interesting (cosine).
- COS (ours) to cosine (distribution) is pretty high, that's good.
- LSA here is the same relation to swow_fsg as before FSG, and less related to this new cosine. 

```{r}
vars = c("swow_fsg", "cosine", "COS", "LSA", "FSG")
cor(master3[ , vars])

#scaling judgments
master3$Judged.Value = master3$Judged.Value / 100

#clean up judgment type
master3$Judgment = gsub("semantic", "Semantic", master3$Judgment)
master3$Judgment = gsub("associative", "Associative", master3$Judgment)
master3$Judgment = gsub("thematic", "Thematic", master3$Judgment)
```

## Hypothesis 1

```{r}
##are average slope and intercept values greater than zero?
##are they within the range of previous work, although maybe controlling for other variables might change them a bit
##frequency of strongest predictor 

####try a loop####
##setup
persontable = matrix(NA,
                     nrow=length(names(table(master3$Partno))),
                     ncol=10)
colnames(persontable) = c("Partno", "AIntercept", "ACOS", "AFSG",
                          "SIntercept", "SCOS", "SFSG",
                          "TIntercept", "TCOS", "TFSG")
simnum = 1

for ( person in names(table(master3$Partno)) ){ ##loop over participants
  
  temp1 = subset(master3, Partno == person & Judgment == "Associative")
  temp2 = subset(master3, Partno == person & Judgment == "Semantic")
  temp3 = subset(master3, Partno == person & Judgment == "Thematic")
  
  persontable[ simnum , 1] = person
  
  if (sum(!is.na(temp1$Judged.Value)) > 9) {
    model9 = lm(Judged.Value ~ cosine + swow_fsg, data = temp1)
    persontable[ simnum , 2:4] = model9$coefficients
  }
  
  if(sum(!is.na(temp2$Judged.Value)) > 9) {
    model10 = lm(Judged.Value ~ cosine + swow_fsg, data = temp2)
    persontable[ simnum , 5:7] = model10$coefficients
  }
  
  if(sum(!is.na(temp3$Judged.Value)) > 9) {
    model11 = lm(Judged.Value ~ cosine + swow_fsg, data = temp3)
    persontable[ simnum , 8:10] = model11$coefficients
  }
  
  simnum = simnum + 1
  
}

##set up the output
people = apply(persontable, 2, as.numeric)
people = as.data.frame(people) 

####hyp 1 are these greater than zero####
##single sample t-test for each column, focus on the effect size
library(MOTE)
library(knitr)

p.value = function(x){
  if (x < .001) { return("< .001")}
  else { return(apa(x, 3, F))}
}

hyp1results = apply(people[ , -1], 2, function (x) { d.single.t(m = mean(x, na.rm = T), 
                                u = 0,
                                sd = sd(x, na.rm = T),
                                n = sum(!is.na(x)),
                                a = .05)})

##what happened? make a table
##create a blank table
tableprint = matrix(NA, nrow = 9, ncol = 8)
colnames(tableprint) = c("Variable", "$M$", "$SD$", "$t$", "$df$", "$p$", "$d$", "$95 CI$")

tableprint[1 , ] = c("Associative Intercept", apa(hyp1results$AIntercept$m, 3, F),
                     apa(hyp1results$AIntercept$sd, 3, F), apa(hyp1results$AIntercept$t, 3, T),
                     hyp1results$AIntercept$df, p.value(hyp1results$AIntercept$p),
                     apa(hyp1results$AIntercept$d, 3, T), paste(apa(hyp1results$AIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$AIntercept$dhigh, 3, T)))

tableprint[2 , ] = c("Associative COS", apa(hyp1results$ACOS$m, 3, F),
                     apa(hyp1results$ACOS$sd, 3, F), apa(hyp1results$ACOS$t, 3, T),
                     hyp1results$ACOS$df, p.value(hyp1results$ACOS$p),
                     apa(hyp1results$ACOS$d, 3, T), paste(apa(hyp1results$ACOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$ACOS$dhigh, 3, T)))

tableprint[3 , ] = c("Associative FSG", apa(hyp1results$AFSG$m, 3, F),
                     apa(hyp1results$AFSG$sd, 3, F), apa(hyp1results$AFSG$t, 3, T),
                     hyp1results$AFSG$df, p.value(hyp1results$AFSG$p),
                     apa(hyp1results$AFSG$d, 3, T), paste(apa(hyp1results$AFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$AFSG$dhigh, 3, T)))

tableprint[4 , ] = c("Semantic Intercept", apa(hyp1results$SIntercept$m, 3, F),
                     apa(hyp1results$SIntercept$sd, 3, F), apa(hyp1results$SIntercept$t, 3, T),
                     hyp1results$SIntercept$df, p.value(hyp1results$SIntercept$p),
                     apa(hyp1results$SIntercept$d, 3, T), paste(apa(hyp1results$SIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$SIntercept$dhigh, 3, T)))

tableprint[5 , ] = c("Semantic COS", apa(hyp1results$SCOS$m, 3, F),
                     apa(hyp1results$SCOS$sd, 3, F), apa(hyp1results$SCOS$t, 3, T),
                     hyp1results$SCOS$df, p.value(hyp1results$SCOS$p),
                     apa(hyp1results$SCOS$d, 3, T), paste(apa(hyp1results$SCOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$SCOS$dhigh, 3, T)))

tableprint[6 , ] = c("Semantic FSG", apa(hyp1results$SFSG$m, 3, F),
                     apa(hyp1results$SFSG$sd, 3, F), apa(hyp1results$SFSG$t, 3, T),
                     hyp1results$SFSG$df, p.value(hyp1results$SFSG$p),
                     apa(hyp1results$SFSG$d, 3, T), paste(apa(hyp1results$SFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$SFSG$dhigh, 3, T)))

tableprint[7 , ] = c("Thematic Intercept", apa(hyp1results$TIntercept$m, 3, F),
                     apa(hyp1results$TIntercept$sd, 3, F), apa(hyp1results$TIntercept$t, 3, T),
                     hyp1results$TIntercept$df, p.value(hyp1results$TIntercept$p),
                     apa(hyp1results$TIntercept$d, 3, T), paste(apa(hyp1results$TIntercept$dlow, 3, T), 
                                                     "-", apa(hyp1results$TIntercept$dhigh, 3, T)))

tableprint[8 , ] = c("Thematic COS", apa(hyp1results$TCOS$m, 3, F),
                     apa(hyp1results$TCOS$sd, 3, F), apa(hyp1results$TCOS$t, 3, T),
                     hyp1results$TCOS$df, p.value(hyp1results$TCOS$p),
                     apa(hyp1results$TCOS$d, 3, T), paste(apa(hyp1results$TCOS$dlow, 3, T), 
                                                     "-", apa(hyp1results$TCOS$dhigh, 3, T)))

tableprint[9 , ] = c("Thematic FSG", apa(hyp1results$TFSG$m, 3, F),
                     apa(hyp1results$TFSG$sd, 3, F), apa(hyp1results$TFSG$t, 3, T),
                     hyp1results$TFSG$df, p.value(hyp1results$TFSG$p),
                     apa(hyp1results$TFSG$d, 3, T), paste(apa(hyp1results$TFSG$dlow, 3, T), 
                                                     "-", apa(hyp1results$TFSG$dhigh, 3, T)))

kable(tableprint)
```

Comparison to original results:
- Intercepts all pretty much the same over estimation effect
- Assoc FSG > stronger / higher than before 
- Assoc cosine about the same
- Semantic FSG > way stronger than before, makes a lot more damned sense
- BECAUSE semantic cosine is now a thing!
- Thematic fsg is about the same but now makes more sense becausee
- Thematic cosine is a thing!

This result tells me:
- interecepts are what we expect .4-.6
- slopes are mostly what we expect
- slopes matter by instruction!

## Hypothesis 2

```{r}
##group them all by type
##dropping partno column
people_thematic = people[ , 8:10]
people_semantic = people[ , 5:7]
people_associative = people[ , 2:4]

##getting absolute values
people_thematic2 = abs(people_thematic)
people_semantic2 = abs(people_semantic)
people_associative2 = abs(people_associative)

##getting rid of the intercepts
people_thematic3 = people_thematic2[ , -1]
people_semantic3 = people_semantic2[ , -1]
people_associative3 = people_associative2[ , -1]

##finding max coefficents for each row
thematic_output = names(people_thematic3)[max.col(people_thematic3, ties.method="first")]
thematic_output = as.data.frame(thematic_output)

semantic_output = names(people_semantic3)[max.col(people_semantic3, ties.method="first")]
semantic_output = as.data.frame(semantic_output)

associative_output = names(people_associative3)[max.col(people_associative3, ties.method="first")]
associative_output = as.data.frame(associative_output)

##sticking it all together
combined = cbind(people$Partno, associative_output, semantic_output, thematic_output)
colnames(combined) = c("Partno", "Associative", "Semantic", "Thematic")

##rename the things
combined$Associative = factor(combined$Associative,
                           labels = c("COS", "FSG"))
combined$Semantic = factor(combined$Semantic,
                           labels = c("COS", "FSG"))
combined$Thematic = factor(combined$Thematic,
                           labels = c("COS", "FSG"))
summary(combined[ , -1])

##what are the percentages on these?
library(memisc)
p1 = percent(combined$Associative)
p1
p2 = percent(combined$Semantic)
p2
p3 = percent(combined$Thematic)
p3

```
- Previously FSG predicted assoc at 64 now 67
- FSG for semantic was highest with  44, now we are seeing even split
- FSG for thematic was 37 with basically nothing for cosine now seeing more even split

## A two level interaction for Hypothesis 2 for JOR

```{r}
library(nlme)

#center the vars
master3$zfsg = scale(master3$swow_fsg, scale = F)
master3$zcosine = scale(master3$cosine, scale = F)

overallh2 = lme(Judged.Value ~ Judgment + 
               zfsg*zcosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2)
```

## What is the interaction

```{r}
#split on cosine because that's what we did before
master3$low_cosine = master3$zcosine + sd(master3$zcosine)
master3$high_cosine = master3$zcosine - sd(master3$zcosine)

overallh2_low = lme(Judged.Value ~ Judgment + 
               zfsg*low_cosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2_low) ##.38

summary(overallh2) ##.30

overallh2_high = lme(Judged.Value ~ Judgment + 
               zfsg*high_cosine, 
             data = master3, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|Partno)

summary(overallh2_high) ##.22 
```

So, what we see is at low levels of cosine, FSG is pulling the weight, but as cosine increases, the effect of FSG decreases (see-sawing).

## Recall for Hypothesis 3

```{r}
library(lme4)

recalloverall = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*zcosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(recalloverall) ##1.63

recalloverall_low = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*low_cosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)

summary(recalloverall_low) #1.89

recalloverall_high = glmer(Recall ~ (1|Partno) + Judgment + 
                        Judged.Value + zfsg*high_cosine,
                      data = master3,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)

summary(recalloverall_high) #1.37
```

Find the exact same pattern!

## Hypothesis 4

```{r}
##merge recall back with slopes from the people dataset
hyp4data = merge(people, master3, by = "Partno")

##association
assoch4 = glmer(Recall ~ (1|Partno) + ACOS + AFSG + AIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##semantics
semh4 = glmer(Recall ~ (1|Partno) + SCOS + SFSG + SIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##thematics
themeh4 = glmer(Recall ~ (1|Partno) + TCOS + TFSG + TIntercept,
                data = hyp4data,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

####table set up####
##create a blank table
tableprint = matrix(NA, nrow = 12, ncol = 5)
colnames(tableprint) = c("Variable", "$b$", "$SE$", "$z$", "$p$")

tableprint[1:4, 2:5] = coef(summary(assoch4))
tableprint[1:4, 1] = dimnames(coef(summary(assoch4)))[[1]]

tableprint[5:8, 2:5] = coef(summary(semh4))
tableprint[5:8, 1] = dimnames(coef(summary(semh4)))[[1]]

tableprint[9:12, 2:5] = coef(summary(themeh4))
tableprint[9:12, 1] = dimnames(coef(summary(themeh4)))[[1]]

library(papaja)
tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] = apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)

kable(tableprint)
```

- Association: we found before that fsg slope predicts best then cosine 
  - Now seeing fsg with no cosine
- Semantics: we found before that cosine predicts best then fsg
  - now seeing same thing 
- Thematics: cosine then negative fsg
  - now seeing fsg then cosine 
  
# TLDR

- Similiar results with maybe less confusion on variables, not mixing types, and no three way interactions!
- Combine thesis and pilot data
  - Still hang on to thesis ideas for part 2 looking at single word variables contributions for a second paper (if you want)
- Switch to this idea of a direct versus indirect relationship?




